{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from bert_utils import AnswerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreSummary = collections.namedtuple(\"ScoreSummary\", [\"short_span_score\", \"cls_token_score\",\n",
    "                                                       \"answer_type_logits\", \"answer_type\",\n",
    "                                                       \"start_logits\", \"end_logits\", \"unique_id\",\n",
    "                                                      \"start_idx_in_chunk\", \"end_idx_in_chunk\"])\n",
    "def empty_score_summary():\n",
    "    return ScoreSummary(None, None, None, None, None, None, None, None, None)\n",
    "    \n",
    "Span = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\", \"score\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_devide(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return a / b\n",
    "\n",
    "def compute_f1(tp, fp, fn):\n",
    "    precision = safe_devide(tp, (tp + fp))\n",
    "    recall = safe_devide(tp, (tp + fn))\n",
    "    f1 = 2 * safe_devide((precision * recall), (precision + recall))\n",
    "    return f1\n",
    "\n",
    "gold_e_by_id_static = {}\n",
    "gold_nq_jsonl_path = '../output/1k/nq-train-part.jsonl'\n",
    "with open(gold_nq_jsonl_path) as gold_nq_jsonl:\n",
    "    for index, line in enumerate(gold_nq_jsonl):\n",
    "        gold_e = json.loads(line, object_pairs_hook=collections.OrderedDict)\n",
    "        gold_e_by_id_static[gold_e['example_id']] = gold_e\n",
    "\n",
    "def compute_score(gold_e_by_id, answers_csv_df, tqdm=None):\n",
    "    \n",
    "    #answers_csv_df = pd.read_csv(answers_csv_path)\n",
    "            \n",
    "    tp_short = 0\n",
    "    fp_short = 0\n",
    "    fn_short = 0\n",
    "    tn_short = 0\n",
    "    tp_long = 0\n",
    "    fp_long = 0\n",
    "    fn_long = 0\n",
    "    tn_long = 0\n",
    "    is_short_fp_column = []\n",
    "    is_long_fp_column = []\n",
    "    \n",
    "    for index, row in answers_csv_df.iterrows():\n",
    "        example_id = row['example_id']\n",
    "        if not example_id in gold_e_by_id:\n",
    "            raise ValueError('example id not found in gold_nq_jsonl file: ' + str(example_id))\n",
    "        annotations = gold_e_by_id[example_id]['annotations']\n",
    "        assert(len(annotations) == 1)\n",
    "        gold_answer = annotations[0]\n",
    "        # short answer\n",
    "        short_answer = row['short_answer']\n",
    "        short_answers_gold = gold_answer['short_answers']\n",
    "        yes_no_answer_gold = gold_answer['yes_no_answer']\n",
    "        #short_answer_score = row['short_answer_score']\n",
    "        is_short_fp = False\n",
    "        is_long_fp = False\n",
    "#         print('short_answer:', short_answer)\n",
    "#         print('short_answers_gold:', short_answers_gold)\n",
    "#         print('yes_no_answer_gold:', yes_no_answer_gold)\n",
    "        \n",
    "        if short_answer == 'YES' or short_answer == 'NO':\n",
    "            # Check for empty gold short answer, otherwise it should be SHORT\n",
    "            ok = short_answer == yes_no_answer_gold and len(short_answers_gold) == 0\n",
    "            tp_short += ok\n",
    "            fp_short += not ok\n",
    "            is_short_fp = not ok\n",
    "#             print(ok)\n",
    "        elif isinstance(short_answer, str):\n",
    "            assert(':' in short_answer)\n",
    "            start_token, end_token = [int(x) for x in short_answer.split(':')]\n",
    "            \n",
    "            ok = len(short_answers_gold) != 0 and \\\n",
    "            start_token == short_answers_gold[0]['start_token'] and \\\n",
    "            end_token == short_answers_gold[-1]['end_token']\n",
    "            \n",
    "            tp_short += ok\n",
    "            fp_short += not ok\n",
    "            is_short_fp = not ok\n",
    "#             print(ok)\n",
    "        elif isinstance(short_answer, float) and math.isnan(short_answer):\n",
    "            # blank short answer\n",
    "            ok = yes_no_answer_gold == 'NONE' and len(short_answers_gold) == 0\n",
    "            tn_short += ok\n",
    "            fn_short += not ok\n",
    "#             print(ok)\n",
    "        else:\n",
    "            raise Exception('wrong short_answer type, short_answer:', short_answer)\n",
    "            \n",
    "        # long answer\n",
    "        long_answer = row['long_answer']\n",
    "        long_answer_gold = gold_answer['long_answer']\n",
    "#         print('long_answer', long_answer)\n",
    "#         print('long_answer_gold', long_answer_gold)\n",
    "        long_answer_gold_str = str(long_answer_gold['start_token']) + ':' + str(long_answer_gold['end_token'])\n",
    "        if isinstance(long_answer, str):\n",
    "            ok = long_answer == long_answer_gold_str\n",
    "            tp_long += ok\n",
    "            fp_long += not ok\n",
    "            is_long_fp = not ok\n",
    "#             print(ok)\n",
    "        elif isinstance(long_answer, float) and math.isnan(long_answer):\n",
    "            # blank long answer\n",
    "            ok = long_answer_gold_str == \"-1:-1\"\n",
    "            tn_long += ok\n",
    "            fn_long += not ok\n",
    "#             print(ok)\n",
    "            \n",
    "#         print('')\n",
    "\n",
    "        \n",
    "        is_short_fp_column.append(is_short_fp)\n",
    "        is_long_fp_column.append(is_long_fp)\n",
    "\n",
    "#     print('short tp:', tp_short, 'fp:', fp_short, 'fn:', fn_short, 'tn:', tn_short,\n",
    "#           'all:', tp_short + fp_short + fn_short + tn_short)\n",
    "#     print('long tp:', tp_long, 'fp:', fp_long, 'fn:', fn_long, 'tn:', tn_long,\n",
    "#          'all:', tp_long + fp_long + fn_long + tn_long)\n",
    "    \n",
    "    f1_short = compute_f1(tp=tp_short, fp=fp_short, fn=fn_short)\n",
    "    f1_long = compute_f1(tp=tp_long, fp=fp_long, fn=fn_long)\n",
    "    \n",
    "    tp = tp_short + tp_long\n",
    "    fp = fp_short + fp_long\n",
    "    fn = fn_short + fn_long\n",
    "    f1 = compute_f1(tp=tp, fp=fp, fn=fn)\n",
    "    \n",
    "    return f1, f1_short, f1_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_short_answer(entry, threshold, check_type):\n",
    "    answer = []    \n",
    "    if entry['answer_type'] == 0:\n",
    "        return \"\"\n",
    "    if check_type and entry['answer_type'] == 4:\n",
    "        return \"\"\n",
    "    \n",
    "    elif entry['answer_type'] == 1:\n",
    "        return 'YES'\n",
    "    \n",
    "    elif entry['answer_type'] == 2:\n",
    "        return 'NO'\n",
    "        \n",
    "    elif entry[\"short_answer_score\"] < threshold:\n",
    "        return \"\"\n",
    "    \n",
    "    else:\n",
    "        for short_answer in entry[\"short_answers\"]:\n",
    "            if short_answer[\"start_token\"] > -1:\n",
    "                answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n",
    "    \n",
    "        return \" \".join(answer)\n",
    "\n",
    "def create_long_answer(entry, threshold, check_type):\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    if entry['answer_type'] == 0:\n",
    "        return ''\n",
    "    if check_type and entry['answer_type'] == 4:\n",
    "        return \"\"\n",
    "    \n",
    "    elif entry[\"long_answer_score\"] < threshold:\n",
    "        return \"\"\n",
    "\n",
    "    elif entry[\"long_answer\"][\"start_token\"] > -1:\n",
    "        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n",
    "        return \" \".join(answer)\n",
    "    \n",
    "def df_long_index_score(df, threshold, check_type):\n",
    "    answers = []\n",
    "    cont = 0\n",
    "    for e in df['long_spans']:\n",
    "        summary = e[3]\n",
    "        if check_type and summary.answer_type == AnswerType.UNKNOWN:\n",
    "            continue\n",
    "        if e[2] > threshold:\n",
    "            index = {}\n",
    "            index['start'] = e[0]\n",
    "            index['end'] = e[1]\n",
    "            index['score'] = e[2]\n",
    "            answers.append(index)\n",
    "            cont += 1\n",
    "        # number of answers\n",
    "        if cont == 1:\n",
    "            break\n",
    "            \n",
    "    return answers\n",
    "\n",
    "def df_short_index_score(df, threshold, check_type):\n",
    "    answers = []\n",
    "    cont = 0\n",
    "    for e in df['short_spans']:\n",
    "        summary = e[3]\n",
    "        if check_type and (summary.answer_type == AnswerType.LONG or\n",
    "                           summary.answer_type == AnswerType.UNKNOWN):\n",
    "            continue\n",
    "        if e[2] > threshold:\n",
    "            index = {}\n",
    "            index['start'] = e[0]\n",
    "            index['end'] = e[1]\n",
    "            index['score'] = e[2]\n",
    "            answers.append(index)\n",
    "            cont += 1\n",
    "        # number of answers\n",
    "        if cont == 1:\n",
    "            break\n",
    "            \n",
    "    return answers\n",
    "\n",
    "def df_example_id(df):\n",
    "    return df['example_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_json_static = pd.read_json(\"../output/1k/predictions.json\")\n",
    "with open(\"../output/1k/predictions.pkl\", 'rb') as f:\n",
    "    predictions_json_static = pickle.load(f)\n",
    "\n",
    "def create_answer(entry):\n",
    "    answer = []\n",
    "    for e in entry:\n",
    "        answer.append(str(e['start']) + ':'+ str(e['end']))\n",
    "    if not answer:\n",
    "        answer = \"\"\n",
    "    return \", \".join(answer)\n",
    "\n",
    "def test_params(threshold_short, check_type_short, threshold0_long, check_type_long):\n",
    "\n",
    "    df_short_index_score_l = lambda q: df_short_index_score(q, threshold=threshold_short,\n",
    "                                                       check_type=check_type_short)\n",
    "    df_long_index_score_l = lambda q: df_long_index_score(q, threshold=threshold0_long, check_type=check_type_long)\n",
    "    \n",
    "    answers_df = pd.DataFrame.from_dict(predictions_json_static)\n",
    "    answers_df['example_id'] = answers_df['predictions'].apply(df_example_id)\n",
    "\n",
    "    answers_df['long_indexes_and_scores'] = answers_df['predictions'].apply(df_long_index_score_l)\n",
    "\n",
    "    answers_df['short_indexes_and_scores'] = answers_df['predictions'].apply(df_short_index_score_l)\n",
    "\n",
    "    answers_df[\"long_answer\"] = answers_df['long_indexes_and_scores'].apply(create_answer)\n",
    "    answers_df[\"short_answer\"] = answers_df['short_indexes_and_scores'].apply(create_answer)\n",
    "    answers_df[\"example_id\"] = answers_df['example_id'].apply(lambda q: str(q))\n",
    "\n",
    "    long_answers = dict(zip(answers_df[\"example_id\"], answers_df[\"long_answer\"]))\n",
    "    short_answers = dict(zip(answers_df[\"example_id\"], answers_df[\"short_answer\"]))\n",
    "\n",
    "    answers_df.example_id = answers_df.example_id.astype('int64')\n",
    "    answers_df = answers_df.replace(r'', np.NaN)\n",
    "    return compute_score(gold_e_by_id_static, answers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_from_range(min_0, max_0, min_1, max_1):\n",
    "    check_type_short = [True]\n",
    "    check_type_long = [True]\n",
    "    delta_0 = (max_0 - min_0) / 20\n",
    "    delta_1 = (max_1 - min_1) / 20\n",
    "    threshold_short_params = np.arange(min_0, max_0, delta_0).tolist()\n",
    "    threshold_long_params = np.arange(min_1, max_1, delta_1).tolist()\n",
    "    short_params = []\n",
    "    long_params = []\n",
    "    for ct in check_type_short:\n",
    "        short_params += [(x, ct) for x in threshold_short_params]\n",
    "    for ct in check_type_long:\n",
    "        long_params += [(x, ct) for x in threshold_long_params]\n",
    "\n",
    "    params = [(*x, *long_params[0]) for x in short_params]\n",
    "\n",
    "    results = []\n",
    "    for p in tqdm(params):\n",
    "        res = test_params(*p)\n",
    "        res_with_params = (res, p)\n",
    "        results.append(res_with_params)\n",
    "    results.sort(reverse=True)\n",
    "    best_short_params = results[0][1][:2]\n",
    "\n",
    "    results = []\n",
    "    params = [(*best_short_params, *x) for x in long_params]\n",
    "    for p in tqdm(params):\n",
    "        res = test_params(*p)\n",
    "        res_with_params = (res, p)\n",
    "        results.append(res_with_params)\n",
    "    results.sort(reverse=True)\n",
    "#     print(results[:5])\n",
    "    return results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.73it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.89it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.82it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first search: score 0.600, params 8.650, 7.300\n",
      "final search: score 0.602, params 8.350, 7.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params_0 = find_best_from_range(1, 10, 1, 10)\n",
    "p0 = params_0[1][0]\n",
    "p1 = params_0[1][2]\n",
    "s = params_0[0][0]\n",
    "params_1 = find_best_from_range(p0 - 1, p0 + 1, p1 - 1, p1 + 1)\n",
    "print('first search:', 'score {0:.3f},'.format(s), 'params {0:.3f},'.format(p0), '{0:.3f}'.format(p1))\n",
    "p0 = params_1[1][0]\n",
    "p1 = params_1[1][2]\n",
    "s = params_1[0][0]\n",
    "print('final search:', 'score {0:.3f},'.format(s), 'params {0:.3f},'.format(p0), '{0:.3f}'.format(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Found best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short tp: 186 fp: 214 fn: 86 tn: 514 all: 1000\n",
      "long tp: 318 fp: 341 fn: 65 tn: 276 all: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5535714285714286, 0.6103646833013435, 0.5880980163360561)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_short = 0.98\n",
    "check_type_short = False\n",
    "threshold_long = 0.83\n",
    "check_type_long = False\n",
    "test_params(threshold_short, check_type_short, threshold_long, check_type_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
